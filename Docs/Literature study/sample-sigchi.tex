%%
%% This is file `sample-sigchi.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigchi')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigchi.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigchi]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
% \AtBeginDocument{%
%   \providecommand\BibTeX{{%
%     \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
\pagestyle{plain} % removes running headers
\usepackage{caption}
\usepackage{svg}
\usepackage{graphics}
\usepackage{amsmath}
\usepackage[british]{babel}
\usepackage{xcolor}
\usepackage{CJKutf8}
% \usepackage{cite}
% \usepackage{graphicx}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{color}
% \usepackage{graphicx}
\usepackage{subcaption}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=C,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}
%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{}
% \copyrightyear{}
% \acmYear{}
% \acmDOI{}

%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Woodstock '18]{Woodstock '18: ACM Symposium on Neural
%   Gaze Detection}{June 03--05, 2018}{Woodstock, NY}
% \acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%   June 03--05, 2018, Woodstock, NY}
% \acmPrice{15.00}
% \acmISBN{978-1-4503-9999-9/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Literature study \\
       Increasing resource utilization of cloud }

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{You Hu}
\email{adolphus.hu@student.vu.nl}
\affiliation{%
  \institution{VU Amsterdam }
%   \city{Amsterdam}
}



%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{You Hu}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.



%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
The cloud computing aims to provide reliable, customized and QoS guaranteed dynamic computing environment for end-users\cite{wang2010cloud}.
The key advantage of cloud computing is the elasticity of resource provisioned on demand. 
It allows developers to deploy their innovative products with much less capital for hardware.
Besides, for companies requiring to process batch tasks, cloud computing provides close to unlimited resources enabling the tasks to get accelerated as long as the program is well-scalable.

From the point of financial concern, it is vital for cloud providers to maximize the resource utilization for their assets.
The first paradigm of cloud computing is based on virtual machine technologies. 
After years of development, containerization as a lighter-weight virtualization technology is used to manage applications widely.      
In this report, we will firstly discuss what is concerned in terms of resource utilization.
Then, we will list researches which aims to maximize resource utilization in virtual machine and container environment respectively.
  
\section{What are concerned in cloud area regarding resource utilization}
\subsection{Definition of resources}

In the cloud environment, there are many kinds of resources and a set of aspects around the cloud economy.
Jennings et al.\cite{Jennings2015} categorize the resources into compute, networking, storage, and power. 
Manvi et al.\cite{Manvi2014} summarize that there are physical resources(CPU, memory, storage, network elements and sensors ) and logical resources(OS, energy, Network throughput/bandwidth, Load balancing mechanisms and so on).
There are overlap between two of them especially in the physical part, while Manvi adds API, OS, load balancing to logical resource concept.
In this report, we mainly focus on the resource optimization on physical resources like CPU, memory, storage; and consider the trade-off between utilization rate and QoS.

\subsection{Quality of Service metrics}
QoS (Quality of Service) metrics are import to both cloud provider and consumer.
Bardsiri and Hashemi listed detailed metrics from four kinds of features:performance, economic, security and general. \cite{Bardsiri2014}
Their coverage is comprehensive. There are plenty of features and corresponding metrics that cloud users would concern.

There are few metrics we consider important in terms of resource utilization. 
For performance features, the CPU load rate and packet loss frequency are what users may concern while the cloud provides needs to make a compromise for utilization of resources.
In the economic aspect, the price per resource unit is the key point that cloud  providers and users wrestle on. However, from the technical view, the time for VM booting/deleting/suspending/provision attract more attention.
Besides, the availability and reliability are very important as well. 
Cloud providers need to pay effort on fault tolerance to make sure the safety of the cloud.
Usually Recovery Point Objective (RPO) and Recovery Time Objective (RTO) are the key parameters for disaster recovery concerned in researches.

In the following sections, we will explore how cloud providers face resource management issues and the metrics shown above play important roles in  those researches.

\section{Virtual machine founded cloud era}

After the first introduction by Eric Schmidt in 2006, cloud computing met its sharp increase.
The development of cloud computing is based on the maturation of virtualization technologies. 
The fine-granularity dividable resource enables users to obtain the 'pay-as-you-go' resources, and data centers can profit from selling every portion of the resource.

More detailed, the cloud users and cloud providers make a formal agreement known as SLA(Service level agreement).
Both cloud providers and cloud consumers need to formulate their management functions regarding the SLAs as for cloud consumers; they also need to meet the SLA requirement to their end-users.
Besides the SLAs, the data center infrastructure-related objectives include load-balancing, fault tolerance, and energy.
While for cloud users, they need to make a trade-off: conservative over-provisioning but less profit or aggressive minimizing cost but a higher risk of violating SLAs.
Considering the resources, computation, networking, storage, power, and etc., academia and industry developed mechanisms or systems to meet SLAs' requirements and achieve better resource management, thus more financially efficient. 
\subsection{Global scheduling}
The most straightforward approach is to optimize the global scheduling of virtualized resources.
Mills et al. compared 18 (heuristics) VM placement algorithms by experiments and parameter grid search\cite{mills2011comparing} in 2011.
The 18 algorithms are made of a combination of 3 criteria for choosing a cluster, times six heuristics for choosing nodes under the two-level taxonomy.
The results reflect the no-free-lunch theorem in optimization: the percent-allocated (PAL) cluster-choice criterion leads to higher average loads and utilization, but this benefit of a cloud provider is based on the negative effect on users, for instance, the waiting time; 
Least-Full first(LF) and Tag\&Pack(TP) lead to lower cloud-wide virtual core utilization as these heuristics more often choose empty nodes on which to place VMs. However, LF  tends to squeeze out some larger VM types, which leads to yield lower user success rate and higher give-up rate.
This comparison gives a benchmark for cloud providers to determine which algorithm to employ. On the other hand, it indicates that the significant outperformance usually exists in the domain-specific scenario.
Recent researches support this trending. The \cite{moges2019energy} focuses on energy aware VM placement; the traffic linked placement solution is purposed in \cite{liwei2020online}; and \cite{kim2019holistic} considers the VM placement in heterogeneous clusters.

\subsection{VM placement}
Following the VM placement, dynamic placement comes to the stage naturally, which benefits from the VM rescaling and live migration.
Sharma et al. \cite{5961733}\cite{5935016} formulated the problems of VM rescaling, replication and lived migration under the cost-efficient scenario as Integer Linear Programming problems
, and proposed Kingfisher, a set of techniques based on greedy heuristic solutions.
This work shows a cost-aware rescaling and lives migration algorithm can reduce the cost of VM transition(paused, serialized, and transferred to a different PM) compared to the cost-oblivious algorithms.
In \cite{6172596}, Wuhib et al. propose a decentralized solution, utilizing a gossip protocol. It is shown to scale to problem sizes above 150,000 PMs and 350,000 VMs and reduces the number of migrations.

Furthermore, the global scheduling for applications and VMs requires resource demand profiling, resource utilization estimation, and resource pricing \& profit maximization as a compliment.
For demand profiling, a typical model-based solution is \cite{gong2010press}, which employs Fast Fourier Transform(FFT) for resource usage detecting and a discrete-time Markov chain for demand predictions.
Furthermore, it was extended by online adaptive padding and reactive error correction to mitigate under-estimation\cite{shen2011cloudscale}.
Resource utilization estimation is not popular in research as the metrics for utilization are very clear.
The considerable research in this topic may relate to the special metric. 
For instance, in \cite{gmach2011chargeback}, a particular CPU and memory-related utilization metrics used by VMware’s Distributed Power Management (DPM) is discussed as it has effects on VMware’s power management mechanism.
In \cite{6847931}, Zhao et al. developed an online algorithm to maximize the profit of the cloud provider over the long run via scheduling job over data centers cross geolocation.

\subsection{Local scheduling}
Following the global scheduling, the local scheduling of VM is vital as well.
The explicit solutions may formulate the problem into the allocation of physical machine resources to VMs.
Urgaonkar et al. proposed an approach aiming to dynamic resource allocation based on queuing information. Therefore the online control is achieve\cite{5488484}.
Each physical machine is equipped with a resource controller and a buffer(queue) containing applications in this approach.
Thus, as it is shown in the work \cite{6195591}, the sequence of the execution of VMs on PM has a significant impact on response time.
The authors propose a local scheduler combining both compute resource allocation and control of VM execution sequencing.

The researches mentioned above are collected due to their representative. Most of them are either published in the early 2010s or very recent.
Overall, these researches reflect a trend that after years of development in the cloud area, resource management research at the VM level has been to an in-depth and much more scenario dependent.
One remarkable exception is that with the rising of deep learning, the state-of-art on old and general topics has a big step forward.
The historical statistic based deep(reinforcement) learning models show substantial advantages on traditional rule-based, fuzzy theory, and heuristic solutions.

\subsection{Provisioning}

\subsection{Workload management}


\section{Container and orchestra}
\subsection{Architecture}
\subsection{Scheduling}
\subsection{Placement}
VMs enable clouds to achieve elasticity of large-scale shared resources, while it is still a heavyweight solution for dynamic provisioning.
Containers as a lightweight technology to virtualize applications have recently been successful and widely used for especially web applications.
Docker is the most popular container solution in the industry at this mount. Therefore, efficient management of the container layer, inserted between VMs and applications, is the must for a container-based cloud.
The management can be implemented directly or on top of container orchestras like Kubernetes or docker swarm.
Researches may either focus on direct management or the management with the help of orchestra frameworks.

The problems can be considered the same as resource management on VM; global scheduling is the most straightforward approach.
Due to the lightweight nature of containerization, the placement of containers can be more flexible.
In \cite{zhang2018container}, the authors proposed architecture for docker container placement.
It especially takes into consideration the collaboration of two kinds(container and VM) of placement.
Normally, The placements of the VM to PM and container to VM are based on the best-fit algorithm.
This work extends the best-fit algorithm, and the placement of containers should consider the resource utilization of PM.
However, this work considers the problem of container placement on VM as a bin pack problem. This assumption makes local scheduling of operating systems useless, and the elasticity of container has been sacrificed.
Another work\cite{7023588} is proposed for container placement as well, which is called Resource Stable Placement(RSP). The difference in architecture is that there is no VM placement problem, only the placement of containers on heterogeneous PMs.
The resources are modeled via a vector where each element represents the amount of one kind of resource.
Therefore, more than CPU and memory can be considered for resource placement.
This scheduling optimization shows better performance in response time and utilization of resources when the workload is heavy.

Besides the direct management, using the orchestra tool, especially like Kubernetes, are the most common approach.
In \cite{medel2016modelling}, the authors developed a Reference net-based model that employs real data from Kubernetes.
It characterizes the performance of Kubernetes and the lifecycle of containers and Pod. 
Kubernetes provides a diverse interface for task management and resource management.
However, as it is mentioned in \cite{wei2018research}, the scheduling only considers the current optimal node, regardless of the use of resource costs.
It provides a better allocation of  Pod scheduling, which is based on the Ant Colony Algorithm and Particle Swarm Algorithm.
The result of experiments on CloudSIM shows resource utilization, and load balancing can be improved significantly.


\newpage  
\bibliography{sample-base.bib}
\bibliographystyle{ACM-Reference-Format}

\end{document}
\endinput
%%
%% End of file `sample-sigchi.tex'.

