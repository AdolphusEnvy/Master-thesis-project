%%
%% This is file `sample-sigchi.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigchi')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigchi.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigchi]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
% \AtBeginDocument{%
%   \providecommand\BibTeX{{%
%     \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
\pagestyle{plain} % removes running headers
\usepackage{caption}
\usepackage{svg}
\usepackage{graphics}
\usepackage{amsmath}
\usepackage[british]{babel}
\usepackage{xcolor}
\usepackage{CJKutf8}
% \usepackage{graphicx}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{color}
% \usepackage{graphicx}
\usepackage{subcaption}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=C,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}
%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{}
% \copyrightyear{}
% \acmYear{}
% \acmDOI{}

%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Woodstock '18]{Woodstock '18: ACM Symposium on Neural
%   Gaze Detection}{June 03--05, 2018}{Woodstock, NY}
% \acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%   June 03--05, 2018, Woodstock, NY}
% \acmPrice{15.00}
% \acmISBN{978-1-4503-9999-9/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Master thesis proposal \\
       A self-adjusted auto-provisioning framework at resource level}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{You Hu}
\email{adolphus.hu@student.vu.nl}
\affiliation{%
  \institution{VU Amsterdam }
%   \city{Amsterdam}
}



%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{You Hu}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}

The Game of Life is a board game. The board consist of N x M cells (N rows, M columns), each having value 1 or 0, depending on whether or not it contains an "organism". 
In each iteration, every cell will change depending on their neighbors. 
This task is time consuming and the cast increase by the size of board.
Therefore, to reduce the time of the task, a parallel solution is proposed. 
The board is divided by the number of board and MPI is used for communication.
The results show that the utilization of multi-processors can achieve great acceleration. 
 
\end{abstract}


%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
\subsection{Game of life}
The cells on the board have eight neighbors. In our case, the world has wrap-around both horizontally and vertically. 
Therefore, left and right borders are connected, and top and bottom borders are connected as well.
In each iteration, the cells will change by the following rules:
\begin{itemize}
    \item Every organism with two or three neighboring organisms survives for the next generation.
    \item Every organism with four or more neighbors dies from overpopulation.
    \item Every organism with one or no neighbor dies from isolation.
    \item Every empty cell adjacent to exactly three occupied neighbor cells will give birth to a new organism. 
\end{itemize}

The computation cost is very high, and the complexity of this task is proportional to the width, the height of board and number of iteration.

when the board is large, it will take too much time to get the results. Therefore, the idea is to use multiple processors to reduce the overall computation time.
\subsection{MPI}
Message Passing Interface (MPI) is a standardized and portable message-passing standard. 
It is a standard instead of a fixed library as it defines the syntax and semantics of a core of library routines.
Therefore, the specific implementation of communication layer can be determined by the infrastructure, which enables application codes achieve portability.

In our case, the MPI implementation is used for point-to-point communication. 
A popular example is $MPI\_Send$, which allows one specified process to send a message to a second specified process. 
The corresponding one is $MPI\_Recv$, together they provide basic point-to-point communication.
Point-to-point operations, as these are called, are particularly useful in patterned or irregular communication, for example, a data-parallel architecture in which each processor routinely swaps regions of data with specific other processors between calculation steps, or a master-slave architecture in which the master sends new task data to a slave whenever the prior task is completed.

In the following section, the specific implementation will be discussed and the details will be explained as well.

\section{Implementation}
\subsection{Sequential implementation}
The sequential version has been provided, our goal is to reduce the time by transforming it to parallel version. 
Before that, the sequential version should be elaborated.

Given a board with $N \times M$ cells(N rows,M columns), the paddings at left, right, top and bottom are added to help make it easier to compute the results for cells at borders.

In the sequential version, processors update the whole board at one time step. 
Since the cells update according to the state of their neighbors at previous time step, besides the table save the current table, there is also an additional table to save the temperer table, they are called as old and new.

In each iteration, the paddings will firstly be updated by the cells on the opposite side of board: left padding is updated by the right most column, right top corner is updated by the left bottom cell and so on.

After that, for each cell on the \textit{old} board the number of live neighbors will be counted. If the summation is 2, then for the corresponding cells on \textit{new} board keep the same as they are on \textit{old} board. 
If the summation is 3, then the corresponding cells on \textit{new} board will be assigned as 1 which indicate there are organism born. Otherwise, they are assigned as 0 which indicate there is no organism or organism dies.

The last step of each iteration is copying the temperer board \textit{new} to the old. 
Besides, it the parameter \textit{print\_world} is enabled and the parameter \textit{nstep} is set as well, the board will be printed by every given steps.

If \textit{print\_world} is set as 0, the number of cells alive will be counted and output to error pipeline.
The final time cast of whole task will be save to log file.

\subsection{Parallel implementation}
The parallel version is based on the idea that the board is divided into multiple parts and each part is handled by different processors in the same time. 
The shared information is passed by MPI between processors. And the final results will be gathered to one processor for output. The details of parallel implementation will be specified in the next subsections.
\subsubsection{The way of division}
Before discussing the way of division, how \textit{new} and \textit{old} board is saved should be elaborated. 

The size of board is defined by two parameters: bwidth and bheight. The board is saved in a 2D array \textit{old} with paddings.
\begin{lstlisting}
  char old[bwidth+2][bheight+2];
  char new[bwidth+2][bheight+2];
\end{lstlisting} 

The 2D array is visualized as Fig.\ref{fig:illustration} to illustrate how the board is saved. As it is shown, the inner rectangle is where the board saved and the outer ring is paddings for computation.
By using the paddings, the same transition method from one step to the next can be applied to every cells in the board.


\begin{figure}[h]

  \includegraphics[scale=0.3]{img/illustration.jpg}
  \centering
  \caption{The example of board}
  \label{fig:illustration}
\end{figure}

According to the way the board is saved, for the cell at $i$ row, $j$ column, its status is save in $old[j][i]$.
Therefore, the information of a complete column is saved in a continuos space of the memory.
To make it easy for communication, the board is divided horizontally. 


\begin{figure}[h]

  \includegraphics[scale=0.4]{img/partition.jpg}
  \centering
  \caption{Horizontal partition}
  \label{fig:partition}
\end{figure}

From Fig.\ref{fig:partition}, the red lines is the borders of division. each processor handle part of the board.

\subsubsection{The number of columns for each processor}

To ensure that every column of board is processed. The number of column each processor should handle is calculated.
Every processor will process part of board from column startCol to column endCol.

The startCol and endCol is formalized as following:

First, assume that there are $N$ processors where $N>0$, for each processor, its identifier and the number of processors can be accessed by the following code.
\begin{lstlisting}
  int world_size;
  MPI_Comm_size(MPI_COMM_WORLD, &world_size);
  int world_rank;
  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
\end{lstlisting}

After the size(width) of board and number of processors is determined, the number of column for the processor with its rank can be computed as $k$.
The startCol and endCol is calculated as below: 

\begin{lstlisting}
  int k=(world_size+(bwidth+2)-1)/world_size;
  k=k>1?k:2;
  startCol=world_rank*k;
  endCol=((world_rank+1)*k)<=bwidth+1?
                          ((world_rank+1)*k)-1:bwidth+1;
\end{lstlisting}

In this code, $k$ is the ceiling division of width of board with paddings and number of processor. 
This ensure that $k*world\_size>=bwidth+2$ and k is a approximate integer number to the average number.

The startCol is simply the $k*world\_rank$ and endCol is startCol plus $k$, but if the endCol is larger than the board boundary, then endCol ends at last column of the board (including paddings).

Thus, for each processor, its part of the board is determined.

\subsubsection{Each iteration of compute}

For each processor, the main computation is similar to the sequential version. According to their given startCol and endCol, each processor only need to process part of board with paddings.

For each iteration (time step), the paddings will be firstly updated. Then the part of board is updated as sequential version.


\subsubsection{Communication}

Since the board is divided into multiple parts and each processor take charge of one of them. 
There is need for communication between processor to exchange the boundary cells. Here, the communication is implemented as following code:

\begin{lstlisting}
  For send side:
      MPI_Send(old[target_col],bheight+2,
        MPI_BYTE,target_rank,current_rank,
        MPI_COMM_WORLD);
  For receive side:
      MPI_Recv(old[target_col],bheight+2,
        MPI_BYTE,target_rank,target_rank,
        MPI_COMM_WORLD,MPI_STATUS_IGNORE);
\end{lstlisting}

To ensure no deadlock during communication, the order of communication is arranged because the communication is blocking version. 
The processors are divided into two groups according to their ranks. The order is shown in Fig. \ref{fig:comm_order}.

As it is shown, in the communication session, the processors with even rank first exchange data with their next processors, and then the previous processors.
The arrows indicate the direction of massage passing. The numbers indicate the order.

\begin{figure}[h]

  \includegraphics[scale=0.5]{img/gol-comm.jpg}
  \centering
  \caption{The order of communication}
  \label{fig:comm_order}
\end{figure}

Note that there is an assumption that the number of processors is even, otherwise there will be a deadlock between the first and last processors.

So far, the details of parallel implementation is elaborated. In the next section, the performance of parallel implementation benchmarked with sequential one will be discussed.

\section{Experiment}

In this section, how parallel implementation accelerate the program is shown by drawing the speedup figure.

\subsection{Theoretical analysis}
There are few parameters that effect the performance: width of board, height of board, number of time step, number of processors(partition).

The total theoretical computation overhead can be formalized as below:
\begin{equation}
  total\_overhead=comp\_overhead + comm\_overhead
  \label{eq:total}
\end{equation}

\begin{equation}
  comp\_overhead=comp\_overhead\_cell\times bheight\times k\times n\_iter 
  \label{eq:comp}   
\end{equation}

\begin{equation}
  comm\_overhead=n\_iter\times p\times bheight\times comm\_overhead\_cell
  \label{eq:comm}
\end{equation}

The equation \ref{eq:total} indicates that the total overhead is summation of computation and communication overhead.

In equation \ref{eq:comp}, the computation overhead is positive linear to : $comp\_overhead\_cell$, the computation cast for one cell, here it is counting the number of live neighbors;
$bheight$, the length of one column; $k$, how many columns one processors should process, while $k$ is approximation of $bwidth/p$ where $p$ is the number of processor;
and $n\_iter$, number of time steps.

For the communication overhead, the equation \ref{eq:comm} shows that it is only related to $p$ and $bheight$.

Therefore, before the experiment, there are few hypothesis can be made according to the equations: 
the computation overhead can be reduced by using multiple processors, and the communication overhead will increase in the same time.

In the following, the results of experiments with different parameter settings will be shown.

\subsection{Performance benchmarking}

In the experiment, the board width, board height and number of iteration are the important parameters.

Because the space of grid search is too large and it is unnecessarily to do grid search, for each testing on one parameter, the other parameters will stay the same.

For each parameter, the overall time under different processors and with sequential version is collected for calculating acceleration.
Besides, for each experiment, there are 3 times of trial for the same parameter setting.

\subsubsection{Board width}

Given board $height=1000$ and $n\_iter=1000$, below are the acceleration and overall compute time with different board width.
\begin{figure}[H]
  %\centering
  \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=\textwidth]{img/acc_bwidth.jpg}
      \caption{Acceleration with different board width}
      \label{fig:acc_bwidth}
  \end{subfigure}

  \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{img/time_bwidth.jpg}
    \caption{Compute time with different board width}
    \label{fig:time_bwidth}
  \end{subfigure}
  \caption{Benchmarking on board width}
  \label{fig:bwidth}
  
\end{figure}

As it is shown in Fig. \ref{fig:bwidth} that the larger the board width is the more the acceleration close to linear acceleration.
Note that, in Fig. \ref{fig:time_bwidth}, the horizontal lines indicate the computation time by sequential version with one machine. 
The lines with same color share same parameter settings. This is the same in the next sections.

\subsubsection{Board height}

Given board $width=1000$ and $n\_iter=1000$, below are the acceleration and overall compute time with different board height.
\begin{figure}[H]
  %\centering
  \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=\textwidth]{img/acc_bheight.jpg}
      \caption{Acceleration with different board height}
      \label{fig:acc_bheight}
  \end{subfigure}
  \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{img/time_bheight.jpg}
    \caption{Compute time with different board height}
    \label{fig:time_bheight}
  \end{subfigure}
  \caption{Benchmarking on board height}
  \label{fig:height}
\end{figure}


As it is shown in Fig. \ref{fig:height} that the larger the height width is the more the acceleration close to linear acceleration.

\subsubsection{Number of iteration}

Given board $width=1000$ and $height=1000$, below are the acceleration and overall compute time with different number of iteration.
\begin{figure}[H]
  %\centering
  \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=\textwidth]{img/acc_n_iter.jpg}
      \caption{Acceleration with different number of iteration}
      \label{fig:acc_n_iter}
  \end{subfigure}
  \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{img/time_n_iter.jpg}
    \caption{Compute time with different number of iteration}
    \label{fig:time_n_iter}
  \end{subfigure}
  \caption{Benchmarking on number of iteration}
  \label{fig:n_iter}
\end{figure}


As it is shown in Fig. \ref{fig:n_iter} that the larger the number of iteration is the more the acceleration close to linear acceleration.

Overall, the results depict the same trend: with the growth of one dimension, the acceleration will be closer to linear acceleration.
This result indicates that according to the Amdahl's law, the growth of communication over head is slower than computation overhead with the increase of overall complexity.
The computation overhead occupies much more than communication overhead. Therefore, the parallelization can achieve very brilliant acceleration.

\section{Conclusion and discussion}

The results of experiment shows that the parallel implementation achieve great acceleration and reduced great overall computation time.
But, there one thing should concern that the sequential version is about 20\%~30\% faster than parallel version with only one processor.
This means that for transplanting sequential implementation to parallel implementation, the additional process(additional branching, variable setting and so on) casts extra computation.
Therefore, even though the parallel implementation achieves close linear acceleration, it is still far away from linear acceleration with the base on sequential version.
There is still space for improvement.

Another thing should be aware is that the job is submitted to multiple nodes, for each node, only one processor is used.
On the one hand, it is waste for computation resources, on the other hand, the communication overhead between nodes may be higher than that between cores.
In the future work, multi-core solution can be explored to reduce communication overhead.

In summary, parallelization can help to reduce the time for computation by utilizing massive computation resources.
MPI is a simple but efficient tool for communication to organize the independent but connected computation nodes.  
%\bibliography{sample-base.bib}
\bibliographystyle{ACM-Reference-Format}
\end{document}
\endinput
%%
%% End of file `sample-sigchi.tex'.
