@article{Farzindar2015,
abstract = {Twitter is among the fastest-growing microblogging and online social networking services. Messages posted on Twitter (tweets) have been reporting everything from daily life stories to the latest local and global news and events. Monitoring and analyzing this rich and continuous user-generated content can yield unprecedentedly valu-able information, enabling users and organizations to acquire actionable knowledge. This article provides a survey of techniques for event detection from Twitter streams. These techniques aim at finding real-world occurrences that unfold over space and time. In contrast to conventional media, event detection from Twitter streams poses new challenges. Twitter streams contain large amounts of meaningless messages and polluted content, which neg-atively affect the detection performance. In addition, traditional text mining techniques are not suitable, because of the short length of tweets, the large number of spelling and grammatical errors, and the frequent use of informal and mixed language. Event detection techniques presented in literature address these issues by adapting techniques from various fields to the uniqueness of Twitter. This article classifies these techniques according to the event type, detection task, and detection method and discusses commonly used features. Finally, it highlights the need for public benchmarks to evaluate the performance of different detection approaches and various features.},
author = {Farzindar, Atefeh and Wael, Khreich},
file = {:D$\backslash$:/work/510/local-branch/SM{\_}topic/literature/5f353daf971093cca46d477861d9ade87c309f8a10287afa99905f298ced2eeb.pdf:pdf},
journal = {Computational Intelligence},
keywords = {Twitter data stream,event detection,event identification,microblogs,monitoring social media},
number = {1},
pages = {132--164},
title = {{a Survey of Techniques for Event Detection in Twitter}},
volume = {31},
year = {2015}
}
@article{Mathioudakis2010,
abstract = {We present TwitterMonitor, a system that performs trend detection over the Twitter stream. The system identifies emerging topics (i.e. 'trends') on Twitter in real time and provides meaningful analytics that synthesize an accurate description of each topic. Users interact with the system by ordering the identified trends using different criteria and submitting their own description for each trend. We discuss the motivation for trend detection over so-cial media streams and the challenges that lie therein. We then describe our approach to trend detection, as well as the architecture of TwitterMonitor. Finally, we lay out our demonstration scenario.},
author = {Mathioudakis, Michael and Koudas, Nick},
doi = {10.1145/1807167.1807306},
file = {:D$\backslash$:/work/510/local-branch/SM{\_}topic/literature/misnis.ref11.pdf:pdf},
isbn = {9781450300322},
issn = {07308078},
journal = {Proceedings of the ACM SIGMOD International Conference on Management of Data},
keywords = {social media analysis,trend detection},
pages = {1155--1157},
title = {{TwitterMonitor: Trend detection over the twitter stream}},
year = {2010}
}
@article{RangarajanSridhar2015,
abstract = {We present an unsupervised topic model for short texts that performs soft clustering over distributed representations of words. We model the low-dimensional semantic vector space represented by the dense distributed representations of words using Gaussian mixture models (GMMs) whose components capture the notion of latent topics. While conventional topic modeling schemes such as probabilistic latent semantic analysis (pLSA) and latent Dirichlet allocation (LDA) need aggregation of short messages to avoid data sparsity in short documents, our framework works on large amounts of raw short texts (billions of words). In contrast with other topic modeling frameworks that use word co-occurrence statistics, our framework uses a vector space model that overcomes the issue of sparse word co-occurrence patterns. We demonstrate that our framework outperforms LDA on short texts through both subjective and objective evaluation. We also show the utility of our framework in learning topics and classifying short texts on Twitter data for English, Spanish, French, Portuguese and Russian.},
author = {{Rangarajan Sridhar}, Vivek Kumar},
doi = {10.3115/v1/w15-1526},
file = {:D$\backslash$:/work/510/local-branch/SM{\_}topic/literature/W15-1526.pdf:pdf},
pages = {192--200},
title = {{Unsupervised Topic Modeling for Short Texts Using Distributed Representations of Words}},
year = {2015}
}
@article{Mehrotra2013,
abstract = {Twitter, or the world of 140 characters poses serious challenges to the efficacy of topic models on short, messy text. While topic models such as Latent Dirichlet Allocation (LDA) have a long history of successful application to news articles and academic abstracts, they are often less coherent when applied to microblog content like Twitter. In this paper, we investigate methods to improve topics learned from Twitter content without modifying the basic machinery of LDA; we achieve this through various pooling schemes that aggregate tweets in a data preprocessing step for LDA. We empirically establish that a novel method of tweet pooling by hashtags leads to a vast improvement in a variety of measures for topic coherence across three diverse Twitter datasets in comparison to an unmodified LDA baseline and a variety of pooling schemes. An additional contribution of automatic hashtag labeling further improves on the hashtag pooling results for a subset of metrics. Overall, these two novel schemes lead to significantly improved LDA topic models on Twitter content. Copyright {\textcopyright} 2013 ACM.},
author = {Mehrotra, Rishabh and Sanner, Scott and Buntine, Wray and Xie, Lexing},
doi = {10.1145/2484028.2484166},
file = {:D$\backslash$:/work/510/local-branch/SM{\_}topic/literature/sigir13.pdf:pdf},
isbn = {9781450320344},
journal = {SIGIR 2013 - Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval},
keywords = {LDA,Microblogs,Topic modeling},
pages = {889--892},
title = {{Improving LDA topic models for microblogs via tweet pooling and automatic labeling}},
year = {2013}
}
@article{AlSumait2009,
abstract = {Topic models, like Latent Dirichlet Allocation (LDA), have been recently used to automatically generate text corpora topics, and to subdivide the corpus words among those topics. However, not all the estimated topics are of equal importance or correspond to genuine themes of the domain. Some of the topics can be a collection of irrelevant words, or represent insignificant themes. Current approaches to topic modeling perform manual examination to find meaningful topics. This paper presents the first automated unsupervised analysis of LDA models to identify junk topics from legitimate ones, and to rank the topic significance. Basically, the distance between a topic distribution and three definitions of "junk distribution" is computed using a variety of measures, from which an expressive figure of the topic significance is implemented using 4-phase Weighted Combination approach. Our experiments on synthetic and benchmark datasets show the effectiveness of the proposed approach in ranking the topic significance. {\textcopyright} 2009 Springer.},
author = {AlSumait, Loulwah and Barbar{\'{a}}, Daniel and Gentle, James and Domeniconi, Carlotta},
doi = {10.1007/978-3-642-04180-8_22},
file = {:D$\backslash$:/work/510/local-branch/SM{\_}topic/literature/ECML09{\_}AlSumaitetal.pdf:pdf},
isbn = {3642041795},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 1},
pages = {67--82},
title = {{Topic significance ranking of LDA generative models}},
volume = {5781 LNAI},
year = {2009}
}
@article{Moussa2018,
abstract = {The volume of data on the social media is huge and even keeps increasing. The need for efficient processing of this extensive information resulted in increasing research interest in knowledge engineering tasks such as Opinion Summarization. This survey shows the current opinion summarization challenges for social media, then the necessary pre-summarization steps like preprocessing, features extraction, noise elimination, and handling of synonym features. Next, it covers the various approaches used in opinion summarization like Visualization, Abstractive, Aspect based, Query-focused, Real Time, Update Summarization, and highlight other Opinion Summarization approaches such as Contrastive, Concept-based, Community Detection, Domain Specific, Bilingual, Social Bookmarking, and Social Media Sampling. It covers the different datasets used in opinion summarization and future work suggested in each technique. Finally, it provides different ways for evaluating opinion summarization.},
author = {Moussa, Mohammed Elsaid and Mohamed, Ensaf Hussein and Haggag, Mohamed Hassan},
doi = {10.1016/j.fcij.2017.12.002},
file = {:D$\backslash$:/work/510/local-branch/SM{\_}topic/literature/1-s2.0-S2314728817300582-main.pdf:pdf},
issn = {23147288},
journal = {Future Computing and Informatics Journal},
keywords = {natural language processing,opinion mining,opinion summarization,sentiment analysis,social media,tweet summarization},
number = {1},
pages = {82--109},
publisher = {Elsevier Ltd},
title = {{A survey on opinion summarization techniques for social media}},
url = {https://doi.org/10.1016/j.fcij.2017.12.002},
volume = {3},
year = {2018}
}
@article{Padmakumar2017,
abstract = {Dense vector representations of words, and more re-cently, sentences, have been shown to improve performance in a number of NLP tasks. We propose a method to perform unsupervised extractive and abstractive text summarization using sentence embeddings. We compare multiple variants of our systems on two datasets, show substantially improved performance over a simple baseline, and performance ap-proaching a competitive baseline.},
author = {Padmakumar, Aishwarya and Saran, Akanksha},
file = {:D$\backslash$:/work/510/local-branch/SM{\_}topic/literature/summarization.pdf:pdf},
title = {{Unsupervised Text Summarization Using Sentence Embeddings}},
url = {http://www.cs.utexas.edu/{~}aish/ut/NLPProject.pdf{\%}0Ahttps://www.cs.utexas.edu/{~}aish/ut/NLPProject.pdf},
year = {2017}
}
@article{Becker,
abstract = {User-contributed messages on social media sites such as Twit-ter have emerged as powerful, real-time means of information sharing on the Web. These short messages tend to reflect a variety of events in real time, earlier than other social media sites such as Flickr or YouTube, making Twitter particularly well suited as a source of real-time event content. In this paper, we explore approaches for analyzing the stream of Twitter messages to distinguish between messages about real-world events and non-event messages. Our approach relies on a rich family of aggregate statistics of topically similar message clusters, including temporal , social, topical, and Twitter-centric features. Our large-scale experiments over millions of Twitter messages show the effectiveness of our approach for surfacing real-world event content on Twitter.},
author = {Becker, Hila and Gravano, Luis},
file = {:D$\backslash$:/work/510/local-branch/SM{\_}topic/literature/2745-14143-1-PB.pdf:pdf},
keywords = {Poster Papers},
pages = {438--441},
title = {{Beyond Trending Topics: Real-World Event Identiﬁcation on Twitter (Tech Report).pdf}},
url = {https://pdfs.semanticscholar.org/2573/060fb7b47e1a69933a28118fc9fd60c393ff.pdf}
}
@article{Ramage2010,
abstract = {As microblogging grows in popularity, services like Twitter are coming to support information gathering needs above and beyond their traditional roles as social networks. But most users' focused on interaction with Twitter their representations of content is for solving still primarily social graphs, forcing the often inappropriate conflation of “people I follow” with “stuff I want to read.” We characterize some information needs that the current Twitter interface fails to support, and argue for better these challenges. We present a scalable implementation of a partially supervised learning model (Labeled LDA) that maps the content of the Twitter feed into dimensions. These dimensions correspond roughly to substance, style, status, and social characteristics of posts. We characterize users and tweets using this model, and present results on two information consumption oriented tasks.},
author = {Ramage, Daniel and Dumais, Susan and Liebling, Dan},
file = {:D$\backslash$:/work/510/local-branch/SM{\_}topic/literature/1528-7850-1-PB.pdf:pdf},
isbn = {9781577354451},
journal = {ICWSM 2010 - Proceedings of the 4th International AAAI Conference on Weblogs and Social Media},
pages = {130--137},
title = {{Characterizing microblogs with topic models}},
year = {2010}
}
@article{Zhang2013,
abstract = {Cross lingual entity linking means linking an entity mention in a background source document in one language with the corresponding real world entity in a knowledge base written in the other language. The key problem is to measure the similarity score between the context of the entity mention and the document of the cand idate entity. This paper presents a general framework for doing cross lingual entity linking by leveraging a large scale and bilingual knowledge base, Wikipedia. We introduce a bilingual topic model that mining bilingual topic from this knowledge base with the assumption that the same Wikipedia concept documents of two different languages share the same semantic topic distribution. The extracted topics have two types of representation, with each type corresponding to one language. Thus both the context of the entity mention and the document of the cand idate entity can be represented in a space using the same semantic topics. We use these topics to do cross lingual entity linking. Experimental results show that the proposed approach can obtain the competitive results compared with the state-of-art approach.},
author = {Zhang, Tao and Liu, Kang and Zhao, Jun},
file = {:D$\backslash$:/work/510/local-branch/SM{\_}topic/literature/6268-31070-1-PB.pdf:pdf},
isbn = {9781577356332},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Natural Language Processing},
pages = {2218--2224},
title = {{Cross lingual entity linking with bilingual topic model}},
year = {2013}
}
@article{Vulic2013,
abstract = {In this paper, we study different applications of cross-language latent topic models trained on comparable corpora. The first focus lies on the task of cross-language information retrieval (CLIR). The Bilingual Latent Dirichlet allocation model (BiLDA) allows us to create an interlingual, language-independent representation of both queries and documents. We construct several BiLDA-based document models for CLIR, where no additional translation resources are used. The second focus lies on the methods for extracting translation candidates and semantically related words using only per-topic word distributions of the cross-language latent topic model. As the main contribution, we combine the two former steps, blending the evidences from the per-document topic dis- tributions and the per-topic word distributions of the topic model with the knowledge from the extracted lexicon. We design and evaluate the novel evidence-rich statistical model for CLIR, and prove that such a model, which combines various (only internal) evidences, obtains the best scores for experiments performed on the standard test collections of the CLEF 2001–2003 campaigns. We confirm these findings in an alternative evaluation, where we automatically generate queries and perform the known-item search on a test subset of Wikipedia articles. The main importance of this work lies in the fact that we train translation resources from comparable document-aligned corpora and provide novel CLIR statistical models that exhaustively exploit as many cross-lingual clues as possible in the quest for better CLIR results, without use of any additional external resources such as parallel corpora or machine-readable dictionaries.},
author = {Vuli{\'{c}}, Ivan and de Smet, Wim and Moens, Marie Francine},
doi = {10.1007/s10791-012-9200-5},
file = {:D$\backslash$:/work/510/local-branch/SM{\_}topic/literature/VulicDeSmetMoensIRJfinal.pdf:pdf},
issn = {13864564},
journal = {Information Retrieval},
keywords = {Cross-language information retrieval,Evidence-rich retrieval models,Probabilistic latent topic models,Unsupervised cross-language lexicon extraction},
number = {3},
pages = {331--368},
title = {{Cross-language information retrieval models based on latent topic models trained with document-aligned comparable corpora}},
volume = {16},
year = {2013}
}
@article{DeSmet2009,
abstract = {...  Cross - lingual news topic  tracking was reported based on cognates (words that are spelled identically over different languages), named en- tities spelled identically and supervised classification with interlingual codes [20]. The ... $\backslash$n},
author = {{De Smet}, Wim and Moens, Marie Francine},
doi = {10.1145/1651437.1651447},
file = {:D$\backslash$:/work/510/local-branch/SM{\_}topic/literature/DeSmetMoensCIKM-SWSM2009.pdf:pdf},
isbn = {9781605588063},
journal = {International Conference on Information and Knowledge Management, Proceedings},
keywords = {Event detection,Latent dirichlet allocation},
pages = {57--64},
title = {{Cross-language linking of news stories on the web using interlingual topic modelling}},
year = {2009}
}
@article{Sasaki2014,
abstract = {Latent Dirichlet allocation (LDA) is a topic model that has been applied to var-ious fields, including user profiling and event summarization on Twitter. When LDA is applied to tweet collections, it gen-erally treats all aggregated tweets of a user as a single document. Twitter-LDA, which assumes a single tweet consists of a single topic, has been proposed and has shown that it is superior in topic semantic coher-ence. However, Twitter-LDA is not capa-ble of online inference. In this study, we extend Twitter-LDA in the following two ways. First, we model the generation pro-cess of tweets more accurately by estimat-ing the ratio between topic words and gen-eral words for each user. Second, we en-able it to estimate the dynamics of user in-terests and topic trends online based on the topic tracking model (TTM), which mod-els consumer purchase behaviors.},
author = {Sasaki, Kentaro and Yoshikawa, Tomohiro and Furuhashi, Takeshi},
file = {:D$\backslash$:/work/510/local-branch/SM{\_}topic/literature/D14-1212.pdf:pdf},
isbn = {9781937284961},
journal = {EMNLP 2014 - 2014 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference},
pages = {1977--1985},
title = {{Online topic model for twitter considering dynamics of user interests and topic trends}},
year = {2014}
}
@article{Lau2012,
abstract = {We present a novel topic modelling-based methodology to track emerging events in microblogs such as Twitter. Our topic model has an in-built update mechanism based on time slices and implements a dynamic vocabulary. We first show that the method is robust in detecting events using a range of datasets with injected novel events, and then demonstrate its application in identifying trending topics in Twitter.},
author = {Lau, Jey Han and Collier, Nigel and Baldwin, Timothy},
file = {:D$\backslash$:/work/510/local-branch/SM{\_}topic/literature/on-line trend.pdf:pdf},
journal = {24th International Conference on Computational Linguistics - Proceedings of COLING 2012: Technical Papers},
keywords = {Online processing,Topic evolution,Topic model,Trend detection,Twitter},
number = {December},
pages = {1519--1534},
title = {{On-line trend analysis with topic models: Twitter trends detection topic model online}},
volume = {2},
year = {2012}
}
@article{Wang2012,
abstract = {Latent topic analysis has emerged as one of the most effective methods for classifying, clustering and retrieving textual data. However, existing models such as Latent Dirichlet Allocation (LDA) were developed for static corpora of relatively large documents. In contrast, much of the textual content on the web, and especially social media, is temporally sequenced, and comes in short fragments, including microblog posts on sites such as Twitter and Weibo, status updates on social networking sites such as Facebook and LinkedIn, or comments on content sharing sites such as YouTube. In this paper we propose a novel topic model, Temporal-LDA or TM-LDA, for efficiently mining text streams such as a sequence of posts from the same author, by modeling the topic transitions that naturally arise in these data. TM-LDA learns the transition parameters among topics by minimizing the prediction error on topic distribution in subsequent postings. After training, TM-LDA is thus able to accurately predict the expected topic distribution in future posts. To make these predictions more efficient for a realistic online setting, we develop an efficient updating algorithm to adjust the topic transition parameters, as new documents stream in. Our empirical results, over a corpus of over 30 million microblog posts, show that TM-LDA significantly outperforms state-of-the-art static LDA models for estimating the topic distribution of new documents over time. We also demonstrate that TM-LDA is able to highlight interesting variations of common topic transitions, such as the differences in the work-life rhythm of cities, and factors associated with area-specific problems and complaints.},
author = {Wang, Yu and Agichtein, Eugene and Benzi, Michele},
doi = {10.1145/2339530.2339552},
file = {:D$\backslash$:/work/510/local-branch/SM{\_}topic/literature/TM-LDA.pdf:pdf},
isbn = {9781450314626},
journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
keywords = {mining social media data,temporal language models,topic transition modeling},
pages = {123--131},
title = {{TM-LDA: Efficient online modeling of latent topic transitions in social media}},
year = {2012}
}
@article{wang2014dirichlet,
  title={A Dirichlet Multinomial Mixture Model-based Approach for Short Text Clustering},
  author={Wang, Jianhua Yin Jianyong},
  year={2014}
}
@article{roderexploring,
  title={Exploring the Space of Topic Coherence Measures},
  author={R{\"o}der, Michael and Both, Andreas and Hinneburg, Alexander}
}